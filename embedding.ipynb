{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_MGJWFPitL3M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: openai in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (1.65.5)\n",
            "Requirement already satisfied: pymysql in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (1.1.1)\n",
            "Requirement already satisfied: sqlalchemy in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (2.0.40)\n",
            "Requirement already satisfied: faiss-cpu in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (1.10.0)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (2.2.3)\n",
            "Requirement already satisfied: tiktoken in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (0.9.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from langchain-community) (0.3.51)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from langchain-community) (0.3.23)\n",
            "Requirement already satisfied: requests<3,>=2 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from langchain-community) (6.0.2)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
            "  Downloading aiohttp-3.11.16-cp310-cp310-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from langchain-community) (0.3.30)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from langchain-community) (2.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from openai) (4.8.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from openai) (2.11.0b1)\n",
            "Requirement already satisfied: sniffio in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from tiktoken) (2024.11.6)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading frozenlist-1.5.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (13 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading multidict-6.4.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading propcache-0.3.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (10 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
            "  Downloading yarl-1.19.0-cp310-cp310-macosx_11_0_arm64.whl.metadata (71 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.31.1 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.31.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/text2sql/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-community) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m889.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.11.16-cp310-cp310-macosx_11_0_arm64.whl (455 kB)\n",
            "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Downloading frozenlist-1.5.0-cp310-cp310-macosx_11_0_arm64.whl (52 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading multidict-6.4.3-cp310-cp310-macosx_11_0_arm64.whl (37 kB)\n",
            "Downloading propcache-0.3.1-cp310-cp310-macosx_11_0_arm64.whl (45 kB)\n",
            "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading yarl-1.19.0-cp310-cp310-macosx_11_0_arm64.whl (94 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: propcache, mypy-extensions, multidict, marshmallow, httpx-sse, frozenlist, attrs, aiohappyeyeballs, yarl, typing-inspect, aiosignal, pydantic-settings, dataclasses-json, aiohttp, langchain-community\n",
            "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 attrs-25.3.0 dataclasses-json-0.6.7 frozenlist-1.5.0 httpx-sse-0.4.0 langchain-community-0.3.21 marshmallow-3.26.1 multidict-6.4.3 mypy-extensions-1.0.0 propcache-0.3.1 pydantic-settings-2.8.1 typing-inspect-0.9.0 yarl-1.19.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-community openai pymysql sqlalchemy faiss-cpu pandas tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JxqDy6vtyHF"
      },
      "source": [
        "**Thư viện**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q9RGzyt9tvkd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.chat_models import ChatAnthropic\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.agents import create_react_agent\n",
        "from langchain_core.messages import HumanMessage\n",
        "from sqlalchemy import create_engine\n",
        "import numpy as np\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "a593Qdu9t1GO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/zg/_wfh70cn569_5p6d24bj1_tr0000gn/T/ipykernel_11543/2369654993.py:15: LangChainDeprecationWarning: The class `ChatAnthropic` was deprecated in LangChain 0.0.28 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-anthropic package and should be used instead. To use it run `pip install -U :class:`~langchain-anthropic` and import as `from :class:`~langchain_anthropic import ChatAnthropic``.\n",
            "  self.llm = ChatAnthropic(model=os.getenv('CLAUDE_3_5_SONNET'), temperature=0)\n"
          ]
        },
        {
          "ename": "ValidationError",
          "evalue": "1 validation error for ChatAnthropic\n  Value error, Did not find anthropic_api_key, please add an environment variable `ANTHROPIC_API_KEY` which contains it, or pass `anthropic_api_key` as a named parameter. [type=value_error, input_value={'model': 'claude-2', 'te...sable_streaming': False}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📌 Answer:\u001b[39m\u001b[38;5;124m\"\u001b[39m, agent\u001b[38;5;241m.\u001b[39mquery(q))\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 112\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[3], line 101\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m--> 101\u001b[0m     agent \u001b[38;5;241m=\u001b[39m \u001b[43mQueryAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     questions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop 3 công ty công nghệ có tăng trưởng cao nhất\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNgành nào có ROE cao nhất 2023?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFPT làm ăn thế nào 5 năm gần đây?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m     ]\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m questions:\n",
            "Cell \u001b[0;32mIn[3], line 15\u001b[0m, in \u001b[0;36mQueryAgent.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM salein_thuc_xuat\u001b[39m\u001b[38;5;124m\"\u001b[39m, con\u001b[38;5;241m=\u001b[39mengine)  \u001b[38;5;66;03m# chỉnh lại table\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# --- 2. Init Embedding & LLM ---\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Khởi tạo mô hình ngôn ngữ\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m \u001b[43mChatAnthropic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCLAUDE_3_5_SONNET\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Embed các trường thông tin thành Vector\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-embedding-3-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/anaconda3/envs/text2sql/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:221\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     emit_warning()\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/text2sql/lib/python3.10/site-packages/langchain_core/load/serializable.py:130\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/text2sql/lib/python3.10/site-packages/pydantic/main.py:243\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    242\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    245\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    249\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    250\u001b[0m     )\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatAnthropic\n  Value error, Did not find anthropic_api_key, please add an environment variable `ANTHROPIC_API_KEY` which contains it, or pass `anthropic_api_key` as a named parameter. [type=value_error, input_value={'model': 'claude-2', 'te...sable_streaming': False}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
          ]
        }
      ],
      "source": [
        "class QueryAgent:\n",
        "    def __init__(self):\n",
        "\n",
        "        # --- 1. Load Database ---\n",
        "        engine = create_engine(\n",
        "            f\"mysql+pymysql://{os.getenv('MYSQL_USER')}:{os.getenv('MYSQL_PASSWORD')}@\"\n",
        "            f\"{os.getenv('MYSQL_HOST')}:{os.getenv('MYSQL_PORT')}/{os.getenv('MYSQL_DB')}\"\n",
        "        )\n",
        "        # data được lưu trong file .env\n",
        "        # đọc dữ liệu từ MySQL database\n",
        "        self.df = pd.read_sql(\"SELECT * FROM salein_thuc_xuat\", con=engine)  # chỉnh lại table\n",
        "\n",
        "        # --- 2. Init Embedding & LLM ---\n",
        "        # Khởi tạo mô hình ngôn ngữ\n",
        "        self.llm = ChatAnthropic(model=os.getenv('CLAUDE_3_5_SONNET'), temperature=0)\n",
        "        # Embed các trường thông tin thành Vector\n",
        "        self.embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "        # --- 3. Create vector for each column ---\n",
        "        self.columns_to_embed = ['name', 'industry', 'description']\n",
        "        self.embedded_columns = {}\n",
        "        for col in self.columns_to_embed:\n",
        "            texts = self.df[col].fillna(\"\").astype(str).tolist()\n",
        "            vectors = self.embedding_model.embed_documents(texts)\n",
        "            self.embedded_columns[col] = np.array(vectors)\n",
        "\n",
        "        # --- 4. Save entire FAISS index (as fallback or hybrid) ---\n",
        "        self.full_texts = self.df[self.columns_to_embed].astype(str).agg(\" \".join, axis=1).tolist()\n",
        "        self.vectorstore = FAISS.from_texts(self.full_texts, embedding=self.embedding_model)\n",
        "\n",
        "    # Đánh trọng số cho các trường thông tin\n",
        "    '''\n",
        "    Đây là bước quan trọng bởi mô hình LLM sẽ dựa vào trọng số để ưu tiên thực hiện Similiarity Search trong Vector DB\n",
        "    '''\n",
        "    def _get_weights_from_llm(self, query: str):\n",
        "        prompt = PromptTemplate.from_template(\"\"\"\n",
        "Bạn là trợ lý phân tích truy vấn người dùng. Dưới đây là truy vấn:\n",
        "\n",
        "\"{query}\"\n",
        "\n",
        "Dựa vào truy vấn trên, hãy gán trọng số (từ 0 đến 1, tổng tối đa là 1) cho các thuộc tính sau:\n",
        "\n",
        "- name\n",
        "- industry\n",
        "- description\n",
        "\n",
        "Trả lời bằng JSON như sau:\n",
        "{{\"name\": 0.3, \"industry\": 0.4, \"description\": 0.3}}\n",
        "        \"\"\")\n",
        "        message = HumanMessage(content=prompt.format(query=query))\n",
        "        result = self.llm.invoke([message])\n",
        "        try:\n",
        "            weights = json.loads(result.content.strip())\n",
        "            return weights\n",
        "        except Exception as e:\n",
        "            print(\"[Warning] LLM weight parsing failed, using default:\", e)\n",
        "            return {\"name\": 0.3, \"industry\": 0.4, \"description\": 0.3}\n",
        "\n",
        "    def _compute_query_vector(self, query, weights):\n",
        "        # Lấy embedding riêng cho từng thuộc tính\n",
        "        vec = np.zeros_like(next(iter(self.embedded_columns.values()))[0])\n",
        "        for col in self.columns_to_embed:\n",
        "            sub_vec = self.embedding_model.embed_query(query)\n",
        "            vec += weights.get(col, 0) * np.array(sub_vec)\n",
        "        return vec\n",
        "\n",
        "    def _search_top_k(self, query_vec, k=5):\n",
        "        # Kết hợp từ các embedding riêng\n",
        "        all_vectors = np.stack([self.embedded_columns[col] for col in self.columns_to_embed], axis=0)\n",
        "        weights = np.array([1.0] * len(self.columns_to_embed))[:, None, None]\n",
        "        merged_vecs = np.average(all_vectors, axis=0, weights=weights.squeeze())\n",
        "        similarities = np.dot(merged_vecs, query_vec) / (\n",
        "            np.linalg.norm(merged_vecs, axis=1) * np.linalg.norm(query_vec)\n",
        "        )\n",
        "        top_indices = similarities.argsort()[::-1][:k]\n",
        "        return self.df.iloc[top_indices]\n",
        "\n",
        "    def query(self, question: str) -> str:\n",
        "        # --- 1. Get weights from LLM ---\n",
        "        weights = self._get_weights_from_llm(question)\n",
        "\n",
        "        # --- 2. Build query vector ---\n",
        "        query_vec = self._compute_query_vector(question, weights)\n",
        "\n",
        "        # --- 3. Search ---\n",
        "        result_df = self._search_top_k(query_vec)\n",
        "\n",
        "        # --- 4. Summarize or use ReAct Agent ---\n",
        "        content = result_df.head(3).to_string(index=False)\n",
        "\n",
        "        system_prompt = f\"\"\"\n",
        "Bạn là chuyên gia tài chính. Đây là dữ liệu phù hợp với truy vấn \"{question}\":\n",
        "{content}\n",
        "\n",
        "Hãy viết câu trả lời cho người dùng, bằng tiếng Việt, ngắn gọn và đầy đủ. Định dạng số liệu với đơn vị nếu cần.\n",
        "        \"\"\"\n",
        "        response = self.llm.invoke([HumanMessage(content=system_prompt)])\n",
        "        return response.content\n",
        "\n",
        "def main():\n",
        "    agent = QueryAgent()\n",
        "    questions = [\n",
        "        \"Top 3 công ty công nghệ có tăng trưởng cao nhất\",\n",
        "        \"Ngành nào có ROE cao nhất 2023?\",\n",
        "        \"FPT làm ăn thế nào 5 năm gần đây?\"\n",
        "    ]\n",
        "    for q in questions:\n",
        "        print(f\"\\n❓ Question: {q}\")\n",
        "        print(\"📌 Answer:\", agent.query(q))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUYNvDjBC7Ls"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "text2sql",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
